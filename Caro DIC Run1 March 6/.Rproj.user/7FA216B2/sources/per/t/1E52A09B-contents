---
title: "190208_OM19_1ml_DIC"
output:
  html_document:
    css: stylesheet.css
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_float: true
    toc_depth: 3
    code_folding: show
    df_print: paged
subtitle: "Source file: 190208_OM19_1ml_DIC.Rmd"
editor_options:
  chunk_output_type: inline
date: "`r Sys.Date()`"
---

# Notes about data reduction and run

This R markdown file uses exponential decay fits of peak amplitudes of GasBench injections to check for proper functioning of the GasBench needle. Only peaks derived from good injections are selected. Exponential fits are then performed on all analyses to project back to the m/z 44 amplitude that would have resulted from an injection occuring at the moment of needle puncture. Expected pCO$_2$ of standards is then calculated from the mass of CaCO$_3$ loaded into vials as well as the volumes of the vial, liquid, and H$_3$PO$_4$ added. A calibration curve is constructed by plotting expected pCO$_2$ of standards vs. their m/z 44 amplitude at t$_0$. $\lbrack\Sigma\text{CO}_2\rbrack$ of the original water sample is calculated based on this calibration. 
Further culling of the data is then performed, and $\delta^{13}$C is corrected for drift, linearity, and isotopic discrimination.

# Setting up and loading packages and files

## Setup

```{r setup, warning=FALSE, message=FALSE}
# load libraries
library(tidyverse) # dplyr, tidyr, ggplot
library(isoreader) # reading isotope data files
library(isoprocessor) # processing isotope data files
library(openxlsx) # reading and writing excel file
library(plotly) # interactive plots
library(knitr) # generating reports
library(ggpmisc) # add equations of best fit to ggplot
library(chemCal) # calculations from calibration curve
```

Data processed with isoreader version `r packageVersion("isoreader")`.

## Load Dataset

```{r load.dataset, message=FALSE}
isofiles <- 
  file.path(
    "~/Google_Drive/Delta methods/GasBench Stuff/Dan/190208_OM19_1ml_DIC/data", "190208_OM19_1ml_DIC_data.cf.rda"
  ) %>% 
  iso_read_continuous_flow()

operator <- "DBN"
session <- "190208_OM19_1ml_DIC"
```

## Chromatograms

Display chromatograms of samples and standards. The first four peaks are reference peaks. The smaller sharp peak after that is a half-inject used to clear the sample loop.

```{r ISO.GC-IRMS.chroms, fig.width=15, fig.height=8}
chroms <- isofiles %>% 
  iso_plot_continuous_flow_data(
    data = c("44"),
    color = `file_id`
  )

chroms %>% ggplotly()
```

## Data Table

Retrieve the data table.

```{r vendor.data.table}
# aggregate vendor data table
data_table <- 
  iso_get_vendor_data_table(isofiles,
  select=c(file_id, rt = Rt, ampl44 = `Ampl 44`, area44 = `Intensity 44`, d13C = `d 13C/12C`, peak_number = `Nr.`), include_file_info = c(analysis = `Analysis`, file_datetime, sample_id = `Identifier 1`, sample_type = `Identifier 2`, mass_loaded_ug = `Comment`, vol_vial_ml = `Preparation`)) 

data_table # print data table
```

# $\lbrack\Sigma\text{CO}_2\rbrack$ calculations

## Quality control of peaks

Filter out reference peaks and first two peaks by their times, and add columns to data frame for the period of time between the GasBench needle puncture the vial and it being injected to the GC-IRMS.
```{r add.t.stab.inject}
raw.data<-subset(data_table, rt > 200) # remove first two peaks which can have contamination from previous injection

sample_transfer_t_s <- 53 # seconds between needle stabbing vial and start of acquisition

rt_CO2_s <- 150 # actual CO2 retention time from start of inject to m/z 44 peak on mass spec

raw.data <- raw.data %>% mutate(t_stab_to_inject_s = rt - rt_CO2_s + sample_transfer_t_s) # make column for time between stabbing of gasbench needle and injection
```

Find and plot a model chromatogram with good injection.
```{r plot.good.chrom}
isofiles %>% iso_filter_files(`Identifier 1` == "205 ug HIS mon2") %>% 
  iso_plot_continuous_flow_data(
    data = c("44"),
    color = `Identifier 1`
  ) %>% ggplotly()
```

Plot the peak amplitudes of the training chromatogram vs. time from start start of He dilution. A non-linear least squares model of exponential decay of m/z 44 amplitude vs. time from needle puncture to inject provides a good fit to the data. This is consistent with physical reality since the sample CO$_2$ is in a constant volume and is being diluted with a constant flow of He.
```{r plot.fit.good.chrom}
training_chrom <- raw.data %>% filter(sample_id == "205 ug HIS mon2")

training_chrom %>% 
   ggplot() +
  aes(
    x = t_stab_to_inject_s,
    y = ampl44
    ) +
  geom_point() +
geom_line(data = function(df) mutate(df, ampl44 = nls(ampl44 ~ A*exp(-k*t_stab_to_inject_s), start = c(A = 10000, k = 0.0001), data = df) %>% predict()), alpha = 0.4)+
    scale_x_continuous(name = "time from GasBench needle puncture until injection on GC-IRMS (s)") +
  scale_y_continuous(name = ("m/z 44 peak amplitude (mV)"))+
  theme_bw()
```

Make non-linear least squares model based on the 'training chromatogram'.
```{r make.nls.model}
exp_model <- nls(data = training_chrom, formula = ampl44 ~ A*exp(-k*t_stab_to_inject_s), start = c(A = 10000, k = 0.0001)) # make and save model

summary(exp_model) # print summary of the model
```

Save exponential decay variable, k
```{r save.k}
k <- as.numeric(coef(exp_model)[2]) # save k

k # print k
```

For demonstration purposes, find and plot a model chromatogram with some bad injections.

We can see here that partial needle clogging, probably due to water condensation on underside of septum, caused some weak injections in this sample.
```{r plot.weak.inject.chrom}
isofiles %>% iso_filter_files(`Identifier 1` == "194 ug YULE drift1") %>% 
  iso_plot_continuous_flow_data(
    data = c("44"),
    color = `Identifier 1`
  ) %>% ggplotly()
```

For demonstrative purposes, now re-plot m/z 44 peak amplitutde vs. period of time between the GasBench needle puncture the vial and it being injected to the GC-IRMS, same chromatogram as above.
```{r plot.weak.peak.inject.ampls}
weak_inject_chrom <- raw.data %>% filter(sample_id == "194 ug YULE drift1")

weak_inject_chrom %>% 
   ggplot() +
  aes(
    x = t_stab_to_inject_s,
    y = ampl44
    ) +
  geom_point()+
    scale_x_continuous(name = "time from GasBench needle puncture until injection on GC-IRMS (s)") +
  scale_y_continuous(name = ("m/z 44 peak amplitude (mV)"))+
  theme_bw()
```


Now, plot all peaks alongside the fit which gives the largest m/z 44 peak amplitude at t$_0$. It is visibly clear which injects were affected by partial needle obstruction.
```{r}
weak_inject_chrom <- weak_inject_chrom %>% mutate(ampl44_t0_per_peak = ampl44 * exp(k*t_stab_to_inject_s)) # For each peak, calculate the projected amplitude at t0 in the chromatogram

weak_inject_chrom_max_ampl44_t0  <- weak_inject_chrom %>% filter(ampl44_t0_per_peak == max(ampl44_t0_per_peak)) # select largest projected amplitude at t0 in the chromatogram
 
max_ampl44_t0 <- weak_inject_chrom_max_ampl44_t0$ampl44_t0_per_peak # save largest projected amplitude at t0

t_max_ampl44_t0 <- weak_inject_chrom_max_ampl44_t0$t_stab_to_inject_s # save the time of the peak with largest projected amplitude at t0
 
weak_inject_chrom_fit_function <- function (x) max_ampl44_t0*exp(-k*x) # write function to calculate ampl44 as a function of t based on the projected amplitude at t0 estimated from the largest peak of the weak inject chromatogram and the k value from the good chromatogram

# plot the peak and the fit based on the point with largest project ampl44 t0
weak_inject_chrom %>%
  ggplot() +
      stat_function(data = data.frame(ampl44=c(1, 600)), aes(x=ampl44), fun = weak_inject_chrom_fit_function, geom="line") +
  aes(
    x = t_stab_to_inject_s,
    y = ampl44
    ) +
  geom_point(color="blue") +
    scale_x_continuous(name = "time from GasBench needle puncture until injection on GC-IRMS (s)", limits = c(0, 600), expand = c(0,0)) +
  scale_y_continuous(name = ("m/z 44 peak amplitude (mV)"))+
  theme_bw()

```

Estimate the expected m/z 44 peak amplitudes of all peaks if proper injections occured based on the maximum projected amplitude at t$_0$ for each analysis. 
```{r estimate.good.peaks}
raw.data_max_peaks <- raw.data %>% mutate(ampl44_t0_per_peak = ampl44 * exp(k*t_stab_to_inject_s)) # For each peak, calculate the projected amplitude at t0 in the chromatogram

raw.data_max_peaks <- raw.data_max_peaks  %>% group_by(file_id) %>% mutate(max_ampl44_t0 = max(ampl44_t0_per_peak)) # add column for largest projected m/z 44 peak amplitude at t0 per peak for a given analysis

raw.data_max_peaks <- raw.data_max_peaks %>% group_by(file_id) %>% mutate(t_of_max_ampl44_t0 = ifelse(ampl44_t0_per_peak == max_ampl44_t0, t_stab_to_inject_s, 0)) # add column for t_stab_to_inject for the peak with largest projected m/z 44 peak amplitude at t0

raw.data_max_peaks <- raw.data_max_peaks %>% group_by(file_id) %>% mutate(t_of_max_ampl44_t0_copied = max(t_of_max_ampl44_t0)) %>% select(-t_of_max_ampl44_t0) # copy t_of_max_ampl44_t0 to whole row and delete previously made column

raw.data_max_peaks <- raw.data_max_peaks %>% group_by(file_id) %>% mutate(ampl44_expected = max_ampl44_t0*exp(-k*t_stab_to_inject_s)) # estimate what the ampl44 would have been based on the peak with largest projected m/z 44 peak amplitude at t0 and previously calculated value of k

raw.data_max_peaks <- raw.data_max_peaks %>% group_by(file_id) %>% mutate(ampl44_expected_plus5percent = ampl44_expected + ampl44_expected*.05) # make column for expected ampl44 + 5%

raw.data_max_peaks <- raw.data_max_peaks %>% group_by(file_id) %>% mutate(ampl44_expected_minus5percent = ampl44_expected - ampl44_expected*.05) # make column for expected ampl44 - 5%
```

Plot measured m/z 44 peak amplitude as well as expected peak amplitude ± 5%. By playing around with the interactive plot, it should be clear that normal injections are consistently within the expected value 5±%, and the bad injections are obviously out of this range. 5% is an arbitrarily selected tuning factor.

```{r plot.expected.peaks.1.percent, fig.width=15, fig.height=8}
ampl44_measured_v_expected <- raw.data_max_peaks %>%
   ggplot() +
  aes(
    x = t_stab_to_inject_s,
    y = ampl44,
    color = file_id,
    label = "measured_ampl44"
    ) +
geom_pointrange(aes(y = ampl44_expected, ymin = ampl44_expected_minus5percent, ymax = ampl44_expected_plus5percent, label = "expected_ampl44"), size = 2, alpha=0.5)+
  geom_point(size=1)+
      scale_x_continuous(name = "time from GasBench needle puncture until injection on GC-IRMS (s)", limits = c(0, 600), expand = c(0,0)) +
  scale_y_continuous(name = ("m/z 44 peak amplitude (mV)"))+
    theme_bw()

ampl44_measured_v_expected %>% ggplotly()
```

### Filter out bad peaks

Filter out peaks that are not within 5% of the expected value for ampl44
```{r filter.peaks}
raw.data_max_peaks_filtered <- raw.data_max_peaks %>% filter(ampl44 > ampl44_expected_minus5percent & ampl44 < ampl44_expected_plus5percent) # filter out peaks that are not within 5% of the expected value for ampl44
```

Plot peaks filtered for good injections
```{r plot.good.peaks, fig.width=15, fig.height=8}
filtered_for_good_injects <- raw.data_max_peaks_filtered  %>%
   ggplot() +
  aes(
    x = t_stab_to_inject_s,
    y = ampl44,
    color = file_id
    )+
        scale_x_continuous(name = "time from GasBench needle puncture until injection on GC-IRMS (s)", limits = c(0, 600), expand = c(0,0)) +
  scale_y_continuous(name = ("m/z 44 peak amplitude (mV)"))+
  geom_point(size=2)+
    theme_bw()

filtered_for_good_injects  %>% ggplotly()
```

Write function for calculating the amplitude of a signal at time 0 given a dataframe of time and signal.
```{r exp.decay.fun}
# function for calculating the amplitude of a signal at time 0 given a dataframe of time and signal
exp_decay_t0 <- function (time, signal, A_guess = 5000, k_guess = k) {
    ampl_t0 <- coef(nls(formula = signal ~ A*exp(-k*time), start = c(A = A_guess, k = k_guess)))[1]
    try(return(ampl_t0))
}
```

Count peaks left after filtering
```{r count.peaks}
peak_counts <- raw.data_max_peaks_filtered  %>% group_by(file_id) %>% summarise(n()) # summarise how many peaks are left after filtering for bad injects

peak_counts #print
```

Only keep samples with at least 4 peaks after quality filtering
```{r keep.at.least.4.peaks}
raw.data_summarise <- raw.data_max_peaks_filtered  %>% group_by(file_id) %>% filter(n()>=4) # only keep samples with at least 4 peaks after quality filtering
```

### Calculate more precisely m/z 44 peak amplitude at t$_{0}$ based on all peaks in an analysis
```{r precise.ampl44.t0.calc}
raw.data_summarise <- raw.data_summarise %>% group_by(file_id) %>% mutate(ampl44_t0 = exp_decay_t0(time = t_stab_to_inject_s, signal = ampl44)) # calculate more exactly ampl44 at t0 based on all peaks in the model
```

### Condense multi-peak dataframe into summary dataframe with one row per analysis.
```{r condense.peaks}
data <- 
  raw.data_summarise %>% 
  group_by(analysis, file_id, sample_id, sample_type, mass_loaded_ug, ampl44_t0) %>% 
  summarize(
    num.peaks=n(),
    d13C.measured=mean(d13C),
    d13C.sd=sd(d13C),
    ampl44_mean=mean(ampl44),
    ampl44.sd=sd(ampl44),
    inv.ampl44=1/ampl44_mean,
    file_datetime=mean(file_datetime)
  )

data <- data %>% ungroup() # print
```

### Calculate limit of quantitation (LOQ)

```{r calc.LOQ}
# Select method blanks

# note: in this run, I included blanks with only H3PO4 and not H3PO4 + water because I only had 1 blank with water and I need at least 2 to calculate LOQ, but still all these blanks are decent representations of true method blanks
method_blanks <- data %>% filter(analysis == 16593 | analysis == 16595 | analysis == 16597)

select(method_blanks, file_id, ampl44_t0) %>% kable()

# mean signal of method blanks
S_mb <- mean(method_blanks$ampl44_t0)

S_mb # print

# standard deviation of signal of method blanks
sd_mb <- sd(method_blanks$ampl44_t0)

sd_mb # print

# calculate the signal for limit of quantitation
# eq. 4.7.4 https://chem.libretexts.org/Bookshelves/Analytical_Chemistry/Book%3A_Analytical_Chemistry_2.0_(Harvey)/04_Evaluating_Analytical_Data/4.7%3A_Detection_Limits
# The ability to detect the analyte with confidence is not the same as the ability to report with confidence its concentration, or to distinguish between its concentration in two samples. For this reason the American Chemical Society’s Committee on Environmental Analytical Chemistry recommends the limit of quantitation, (SA)LOQ.

S_A_LOQ <- S_mb + 10 * sd_mb

S_A_LOQ # print 
```

Check if data is quantifiable
```{r check.above.LOQ}
# insert dummy row for LOQ
data <- data %>% add_row(file_id = "LOQ", sample_id = "LOQ", sample_type = "sample", ampl44_t0 = S_A_LOQ)

# add column for samples above or below limit of quantitation
data <- data %>% mutate(LOQ = ifelse(ampl44_t0 >= S_A_LOQ, "quantifiable", "BLOQ"))

# select relevant data and print
data %>% select(file_id, ampl44_t0, LOQ) %>% kable()
```

## Create calibration curve

Correct data types for calculations
```{r transform.data.types}
data <- transform(data, mass_loaded_ug = as.numeric(mass_loaded_ug)) # transform mass_loaded data type from character to numeric

data <- data %>% mutate(file_datetime_num = as.numeric(file_datetime)) # add a column for the datetime in numeric format (i.e. seconds since 1970-01-01) for later use in drift corrections
```

## Adjust some constants depending on sample preparation. *User input needed.*
```{r add.constants}
vol_vial_ml = 11.7 # volume of Exetainer with septum screwed down
vol_H2O_sample_ml <- 1 # volume of water sample in ml
vol_H3PO4_added_ml <- .1 # volume of concentrated H3PO4 added to samples and standards
```

select standards for calibration curve
```{r select.calib.stnds}
linC <- data %>% filter(sample_type == "lin.std") # filter for linearity standards
```

Plot calib curve based on mass loaded
```{r plot.calib.curve.mass.loaded}
calib_DIC <-
ggplot (linC, aes(x=mass_loaded_ug, y=ampl44_t0, label = num.peaks)) +
  geom_point() +
          scale_x_continuous(name = "mass CaCO3 loaded (µg)") +
  scale_y_continuous(name = ("m/z 44 peak amplitude t0 (mV)"))+
  theme_bw()

ggplotly(calib_DIC)
```

Something is obviously wrong with the 435µg standard. Perhaps an error in weighing and transferring the standard, or piece of standard got stuck on the side of Exetainer and did not dissolve, or cap was not properly closed, allowing CO$_2$ to diffuse out. The chromatogram below looks fine, so it's unclear what the problem was.
```{r plot.weird.stnd.chrom}
isofiles %>% iso_filter_files(`Identifier 1` == "435 ug YULE lin") %>% 
  iso_plot_continuous_flow_data(
    data = c("44"),
    color = `Identifier 1`
  ) %>% ggplotly()
```

Cull 435µg YULE from linearity standards
```{r cull.435ug.YULE}
linC <- linC %>% filter(sample_id != "435 ug YULE lin") # something obviously wrong with 435 ug sample 
```

Replot calibration curve with 435 µg standard culled
```{r plot.calib.curve.mass.loaded.culled}
calib_mass_loaded_summ <- summary(lm(linC$ampl44_t0 ~ linC$mass_loaded_ug)) # summarize regression of m/z 44 amplitude at t0 vs. mass loaded

calib_DIC_2  <- 
  ggplot(linC, aes(x=mass_loaded_ug, y=ampl44_t0)) +
  geom_smooth(method="lm", color = "blue") +
  geom_point(shape=21, fill="black", size = 2)+
  stat_poly_eq(aes(label =  paste(stat(eq.label), stat(rr.label), sep = "~~~~")),
               formula = linC$ampl44_t0 ~ linC$mass_loaded_ug , parse = TRUE, rr.digits = 6, color = "blue")+
 scale_x_continuous(name = latex2exp::TeX("mass CaCO$_3$ loaded (µg)"))+
 scale_y_continuous(name = latex2exp::TeX("m/z 44 peak amplitude t$_0$ (mV)"))+
theme_bw()

calib_DIC_2
```

Preparing constants and equations to calculate pCO$_2$ from µg CaCO$_3$
```{r data.for.pCO2}
# calculating henry's constant at lab conditions

R <- 0.083144598 # R (l * bar * K−1 * mol−1)
Pa_bar  <- 1e5 # Pa/bar
l_m3 <- 1e3 # l m^-3

Hcp_CO2_25C_DI <- 3.30E-04 # Henry's constant (Hcp) @ 298.15K in deonized water (Sander, 2015)[mol m^-3 Pa^-1]
#eqn:  Hcc = c(aq) / c(g)
#Hcc = Hcp * R * T
Hcp_lit_temp_K <- 298.15 # temp in K of literature henry constant

Hcp_CO2_25C_DI_bar <- Hcp_CO2_25C_DI * Pa_bar / l_m3 # Hcp mol L^-1 bar^-1
Hcc_CO2_25C_DI <- Hcp_CO2_25C_DI_bar * R * Hcp_lit_temp_K # dimensionless Hcc

Hcp_temp_correct_factor  <- 2400 #dlnHcp/d(1/T) [K] temperature correction factor (Sander, 2015)

lab_temp_C <- 21
lab_temp_K <- lab_temp_C + 273.15

Hcp_CO2_lab_temp_DI <- Hcp_CO2_25C_DI * exp(Hcp_temp_correct_factor * (1/lab_temp_K - 1/Hcp_lit_temp_K)) #Henry constant at lab temp in DI water [mol m^-3 Pa^-1]

Hcp_CO2_lab_temp_DI_bar <- Hcp_CO2_lab_temp_DI * Pa_bar / l_m3 # Hcp mol L^-1 bar^-1

Hcc_CO2_lab_temp_DI <- Hcp_CO2_lab_temp_DI_bar * R * lab_temp_K # dimensionless Hcc

PO4_stock_M <- 14.8 # moles / liter of phosphate in concentrated stock sol'n (85 wt %) https://www.sigmaaldrich.com/chemistry/stockroom-reagents/learning-center/technical-library/reagent-concentrations.html

vol_l_ml <- vol_H2O_sample_ml + vol_H3PO4_added_ml # volume of water + acid in ml

water_H3PO4_ratio <- vol_H2O_sample_ml / vol_H3PO4_added_ml    # ratio of concentrated H3PO4 (85 wt%) to water in DIC prep method
dilution_factor_H3PO4 <- (1/(1+1*water_H3PO4_ratio)) # dilution factor of concentrated H3PO4 during acidification of water sample

ci  <- 14.8 * dilution_factor_H3PO4 # concentration of total phosphate and its protonated forms in acidified water sample [kmol m^-3 aka mol/l]
hi_H2PO4 <- 0.1025 # m^3kmol^-1 ion-specific parameter (schumpe 1993)
hg_CO2 <- -0.0183 # m^3kmol^-1 gas-specific parameter (schumpe 1993)

Hcc_CO2_lab_temp_and_ionic_strength <- Hcc_CO2_lab_temp_DI * 10^-((hi_H2PO4 + hg_CO2) * ci)
```

Calculate expected pCO$_2$ from µg CaCO$_3$ loaded
```{r calc.pCO2}
MM_CaCO3 <- 100.0869 #g/mol

linC <- linC %>% mutate(mol_CO2_total_expected = mass_loaded_ug * 1e-6 / MM_CaCO3) # add column for total moles CO2 expected

linC <- linC %>% mutate(mol_ratio_CO2_g_aq = (vol_vial_ml - vol_l_ml) * 1e-3 / (vol_l_ml*1e-3 * Hcc_CO2_lab_temp_and_ionic_strength)) # add column for mole ratio of CO2 gas / aqueous

linC <- linC %>% mutate(mol_CO2_g = mol_CO2_total_expected / (1+ (1/mol_ratio_CO2_g_aq))) # add column for total moles CO2 in gas phase

linC <- linC %>% mutate(p_CO2_expected_bar = mol_CO2_g * R * lab_temp_K / ((vol_vial_ml - vol_l_ml)*1e-3)) # add column for expected pCO2

```

### Re-plot calibration curve in terms of pCO$_2$

```{r plot.calib.curve.pCO2}
calib_DIC_3  <- 
  ggplot(linC, aes(x=p_CO2_expected_bar, y=ampl44_t0)) +
  geom_smooth(method="lm", color = "blue") +
  geom_point(shape=21, fill="black", size = 2)+
  stat_poly_eq(aes(label =  paste(stat(eq.label), stat(rr.label), sep = "~~~~")),
               formula = linC$ampl44_t0 ~ linC$p_CO2_expected_bar , parse = TRUE, rr.digits = 6, color = "blue")+
   scale_x_continuous(name = latex2exp::TeX("pCO$_2$ expected (bar)"))+
 scale_y_continuous(name = latex2exp::TeX("m/z 44 peak amplitude t$_0$ (mV)"))+
theme_bw()

calib_DIC_3 # show plot
```

```{r calc.calib.fit.pCO2}
calib_fit_pCO2 <- lm(linC$ampl44_t0 ~ linC$p_CO2_expected_bar) # make linear regression of m/z 44 amplitude at t0 vs. mass loaded

calib_fit_summ_pCO2  <- summary(calib_fit_pCO2) # summarize regression statistics

calib_fit_summ_pCO2 # print regression statistics
```

### Calculate pCO$_2$ of samples

Add more general names to dataset for combining replicates
```{r add.names}
samples<-filter(data, sample_type=="sample") # filter for samples

samples <- samples  %>% mutate(sample_name = case_when(
  str_detect(file_id, "16591__5 min He purge 190104") == TRUE ~ "5 min He purge 190104",
  file_id == "16593__5 min He purge 190104 + 100 ul H3PO4 190104 rep1.dxf" ~ "100 ul H3PO4 190104",
    file_id == "16595__5 min He purge 190104 + 100 ul H3PO4 190104 rep3.dxf" ~ "100 ul H3PO4 190104",
  analysis == 16597 ~ "1 ml boiled MilliQ 190104",
  analysis == 16617 ~ "1 ml boiled MilliQ 190104 + 194ug YULE",
  str_detect(file_id, "WAB71") == TRUE ~ "WAB71",
  str_detect(file_id, "BA1A@30") == TRUE ~ "BA1A<30",
  str_detect(file_id, "WAB105") == TRUE ~ "WAB105",
  str_detect(file_id, "NSHQ14") == TRUE ~ "NSHQ14",
  str_detect(file_id, "BA1A4165") == TRUE ~ "BA1A4165",
  str_detect(file_id, "BA1A108132") == TRUE ~ "BA1A108132",
  str_detect(file_id, "LT17-W5") == TRUE ~ "LT17-W5",
  str_detect(file_id, "LT17-W6") == TRUE ~ "LT17-W6",
  str_detect(file_id, "LT17-W3") == TRUE ~ "LT17-W3",
  str_detect(file_id, "LOQ") == TRUE ~ "LOQ"
))
```

### Apply calibration
```{r apply.calib.inverse.predict}
# use inverse.predict function from chemCal to predict X based on Y
samples_calibrated <- samples %>% group_by(sample_name) %>%  mutate(pCO2_bar = as.numeric(inverse.predict(object = calib_fit_pCO2, newdata = ampl44_t0, alpha = 0.05)[1]))

# use inverse.predict function from chemCal to calculate the 95% confidence interval for the prediction
samples_calibrated <- samples_calibrated %>%  mutate(pCO2_bar_95_confidence = as.numeric(inverse.predict(object = calib_fit_pCO2, newdata = ampl44_t0, alpha = 0.05)[2]))

# summarize calibrated samples 
samples_calibrated_summ <- samples_calibrated %>% group_by(sample_name) %>% summarise(n = n(), ampl44_t0 = mean(ampl44_t0), pCO2_bar = first(pCO2_bar), pCO2_bar_95_confidence = first(pCO2_bar_95_confidence), LOQ = ifelse(any(LOQ == "BLOQ") == TRUE, "BLOQ", "quantifiable"))

samples_calibrated_summ %>% kable(digits = 6) # print
```

Now, just for demonstration purposes, convert mass CaCO$_3$ loaded to $\lbrack\Sigma\text{CO}_2\rbrack$ and re-plot. This is a simpler, more common, and slightly less exact/representative way too make such a calibration curve.
```{r calc.DIC.stnds}
MM_CaCO3 <- 100.0869 #g/mol

linC <- linC %>% mutate(mol_CO2_total_expected = mass_loaded_ug * 1e-6 / MM_CaCO3) # add column for total moles CO2 expected

linC <- linC %>% mutate(DIC_uM = mol_CO2_total_expected / (vol_H2O_sample_ml *1e-3) * 1e6) # dissolved inorganic carbon concentration of initial water sample by dividing total moles CO2 by volume of water

calib_DIC_4  <- 
  ggplot(linC, aes(x=DIC_uM, y=ampl44_t0)) +
  geom_smooth(method="lm", color = "blue") +
  geom_point(shape=21, fill="black", size = 2)+
  stat_poly_eq(aes(label =  paste(stat(eq.label), stat(rr.label), sep = "~~~~")),
               formula = linC$ampl44_t0 ~ linC$DIC_uM , parse = TRUE, rr.digits = 6, color = "blue")+
 scale_x_continuous(name = latex2exp::TeX("estimated $\\lbrack$$\\Sigma$CO$_2$$\\rbrack$ (µM)"))+
 scale_y_continuous(name = latex2exp::TeX("m/z 44 peak amplitude t$_0$ (mV)"))+
  theme_bw()

calib_DIC_4

# make interactive plot
calib_DIC_5  <- 
  ggplot(linC, aes(x=DIC_uM, y=ampl44_t0, label=sample_id))+
  geom_point()+
theme_bw()

calib_DIC_5 %>% ggplotly()
```

### Calculate $\lbrack\Sigma\text{CO}_2\rbrack$ of samples
```{r calc.DIC.samples}
### for concentration

samples_calibrated_summ <- samples_calibrated_summ %>% mutate(mol_CO2_g = pCO2_bar * (vol_vial_ml - vol_l_ml) * 1e-3 / (R * lab_temp_K)) # calculate moles CO2 in gas phase

samples_calibrated_summ <- samples_calibrated_summ %>% mutate(mol_CO2_aq = mol_CO2_g * vol_l_ml*1e-3 * Hcc_CO2_lab_temp_and_ionic_strength / ((vol_vial_ml - vol_l_ml)*1e-3)) #calculated moles CO2 in aqueous phase

samples_calibrated_summ <- samples_calibrated_summ %>% mutate(mol_CO2_tot = mol_CO2_g + mol_CO2_aq) # sum aqueous and gaseous CO2

samples_calibrated_summ <- samples_calibrated_summ %>% mutate(DIC_uM = mol_CO2_tot / (vol_H2O_sample_ml * 1e-3) * 1e6) # convert total moles CO2 to dissolved inorganic C concentration of initial water sample

### for confidence interval

samples_calibrated_summ <- samples_calibrated_summ %>% mutate(mol_CO2_g_95_confidence = pCO2_bar_95_confidence * (vol_vial_ml - vol_l_ml) * 1e-3 / (R * lab_temp_K)) # calculate moles CO2 in gas phase

samples_calibrated_summ <- samples_calibrated_summ %>% mutate(mol_CO2_aq_95_confidence = mol_CO2_g_95_confidence * vol_l_ml*1e-3 * Hcc_CO2_lab_temp_and_ionic_strength / ((vol_vial_ml - vol_l_ml)*1e-3)) #calculated moles CO2 in aqueous phase

samples_calibrated_summ <- samples_calibrated_summ %>% mutate(mol_CO2_tot_95_confidence = mol_CO2_g_95_confidence + mol_CO2_aq_95_confidence) # sum aqueous and gaseous CO2

samples_calibrated_summ <- samples_calibrated_summ %>% mutate(DIC_uM_95_confidence = mol_CO2_tot_95_confidence / (vol_H2O_sample_ml * 1e-3) * 1e6) # convert total moles CO2 to dissolved inorganic C concentration of initial water sample

### clean and print
samples_select <- samples_calibrated_summ %>% select(sample_name, n, ampl44_t0, pCO2_bar, pCO2_bar_95_confidence, DIC_uM, DIC_uM_95_confidence, LOQ) 

samples_select %>% kable(caption = "[DIC] from calibration of ampl44 t0 vs. pCO2 expected")
```

Plot samples to check that their amplitude roughly makes sense given their calculated $\lbrack\Sigma\text{CO}_2\rbrack$. Note that on this plot, the standards' $\lbrack\Sigma\text{CO}_2\rbrack$ was calculated simply with the mass loaded, whereas the samples were calculated based on the pCO$_2$ calibration. The samples do plot as expected.

Dashed line = LOQ.

```{r ampl44.DIC.sample.stnd.check}
LOQ_DIC_um <- as.numeric(samples_select %>% filter(sample_name == "LOQ") %>% select(DIC_uM))

ampl44.DIC.sample.stnd.check <- ggplot(samples_calibrated_summ, aes(x=ampl44_t0, y=DIC_uM, label=sample_name, color = "samples")) +
  geom_hline(yintercept = LOQ_DIC_um, linetype = "dashed", alpha = 0.3)+
  geom_point()+
  geom_point(data = linC, aes(x=ampl44_t0, y=DIC_uM, label=file_id, color="standards")) +
            scale_y_continuous(name = "DIC (µM)") +
  scale_x_continuous(name = ("m/z 44 amplitude t0 (mV)"))+
  theme_bw()

ampl44.DIC.sample.stnd.check %>% ggplotly()
```

# $\delta^{13}$C corrections

## Initial dataset checks, plots, and culling

First round of culling looks at standard deviations of the stable isotope values of the individual peaks (typically there are 9 peaks, prior to previous culling for bad injects), and uses that information to identify analytical outliers or samples with problems or too few peaks. Often the "cutoff" of outliers tends to be sd of 0.075 - 0.1 permil.

Make summary plots of reproducibility of isotopic values.
```{r plot.stnds}
data <- data %>% filter(ampl44_t0 >= S_A_LOQ) # filter for samples with signal greater than or equal LOQ for concentration

sd.hist<-ggplot(data, aes(x=d13C.sd, fill = ampl44_mean)) +
  geom_histogram(binwidth=.01) +
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

sd.values<-ggplot(data, aes(x=analysis, y=d13C.sd, label=sample_id, color = ampl44_mean)) +
  geom_point() +
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  scale_color_gradientn(colours = c("red", "blue", "blue"), values = c(0, 0.2, 1))

sd.v.ampl44<-ggplot(data, aes(x=ampl44_mean, y=d13C.sd, fill=factor(num.peaks), label=file_id)) +
  geom_point(size=3, shape=21)+
  theme_bw()+
  scale_fill_discrete(name="# peaks")

sd.hist
sd.values %>% ggplotly()
sd.v.ampl44 %>% ggplotly()
```

Remove any data points that did not replicate within uncertainty for the individual peaks, redo plots - creates a "culled data" file that shows samples and standards that shouldn't be used
```{r cull.outliers.1}
d13C.sd.cutoff <- 0.075 # set a standard deviation on d13C measurements between peaks that you deem acceptable

culled.data <- subset(data, d13C.sd>d13C.sd.cutoff)  # subset data that don't meet the acceptability threshold for d13C standard deviation
wo.culled <- subset(data, d13C.sd<d13C.sd.cutoff) # subset data that do meet the threshold

stds1<- subset(wo.culled, sample_type!="sample" & sample_type!="Flush Gas")

sd.hist.wo.culled<-ggplot(wo.culled, aes(x=d13C.sd, fill = ampl44_mean)) +
  geom_histogram(binwidth=.01) +
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

sd.values.wo.culled<-ggplot(wo.culled, aes(x=analysis, y=d13C.sd, label=sample_id, color = ampl44_mean)) +
  geom_point() +
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  scale_color_gradientn(colours = c("red", "blue", "blue"), values = c(0, 0.2, 1))

sd.v.ampl44.wo.culled<-ggplot(wo.culled, aes(x=ampl44_mean, y=d13C.sd, fill=factor(num.peaks), label=file_id)) +
  geom_point(size=3, shape=21)+
  theme_bw()+
  scale_fill_discrete(name="# peaks")


sd.hist.wo.culled
sd.values.wo.culled %>% ggplotly()
sd.v.ampl44.wo.culled %>% ggplotly()
```

Plot yields of everything, and then just the standards, using INTERACTIVE plots. Use this to cull more samples if need be, by looking for statistical outliers that coincide with yield problems.
Note that the linear model in the yield.stds figure here is based on ALL the standard data, so ALL stds that fall far off the line should be excluded from corrections

Check to see that samples all fall within linearity range, and for any other obvious outliers
to check:

* do linearity standards cover the area space of your samples and other standards?
* do all the standards show typical trends between area and mass? Do you see very general trends like that in your samples?

```{r cull.outliers.2, fig.height=8, fig.width=10}
stds1 <- stds1 %>% mutate(standard = case_when(
  sample_type == "drift.std" | sample_type == "lin.std" | sample_type == "dis.std.1" ~ "YULE",
  sample_type == "mon.std" | sample_type == "dis.std.2" ~ "HIS",
  sample_type == "dis.std.3" ~ "LSVEC"
))

yield.stds<-ggplot(stds1, aes(x=mass_loaded_ug, y=ampl44_mean, label=file_id)) +
  stat_smooth(method="lm") +
  geom_point(aes(color=standard)) +
  theme_bw()

# note: LSVEC is LiCO3, not CaCO3, so it is expected to be above the yield of the other standards

d13C.stds <- 
  ggplot(stds1, aes(label=file_id)) +
  geom_point(shape=21, mapping = aes(x=ampl44_mean, y=d13C.measured, fill=standard)) +
  facet_grid(standard ~ ., scales = "free") +
  theme_bw()

ggplotly(yield.stds)
ggplotly(d13C.stds )
```

```{r cull.outliers.3, fig.height=8, fig.width=10}
# adjust next line only: change #'s in stds.to.cull to reflect the analysis #'s that need to be culled, add analysis #'s as needed and rerun after looking at the new plots
stds.to.cull <- c(16609) # analyses with yield problem and/or significant outlier in d13C
                    # bad yield for analysis 16609

stds.culled <- filter(stds1, analysis %in% stds.to.cull)
culled.data<-bind_rows(culled.data, stds.culled)

stds2 <- filter(stds1, !analysis %in% stds.to.cull)
wo.culled <- filter(wo.culled, !analysis %in% stds.to.cull)

yield.stds2<- stds2 %>% filter(standard != "LSVEC") %>% ggplot(aes(x=mass_loaded_ug, y=ampl44_mean, label=file_id)) +
  stat_smooth(method="lm") +
  geom_point(aes(color=standard)) +
  theme_bw()

d13C.stds2 <- 
  ggplot(stds2, aes(label=analysis)) +
  geom_point(shape=21, mapping = aes(x=ampl44_mean, y=d13C.measured, fill=standard)) +
  facet_grid(standard ~ ., scales = "free") +
  theme_bw()

ggplotly(yield.stds2)
ggplotly(d13C.stds2)
```

## Apply corrections to quality-filtered data

### Define standards

```{r define.stnds}
#input all standard values in VPDB mineral values; be sure to adjust for YOUR standards

corr.std<-"YULE"    #you should type the name of whichever standard you used for both your lin.std and drift.std here
C.acc <- -2.28 

dis.std<-"LSVEC"
C.acc.dis <- -46.6

mon.std<-"HIS"
C.acc.mon <- -4.80

# data frame overview
C.stds.table <- 
  tribble( # data frame defined by Row, instead of by column
    ~std.name, ~C.acc, # define the column names
    corr.std, C.acc,   # add Row one
    mon.std, C.acc.mon,# add Row two
    dis.std, C.acc.dis # add Row three, etc.
  )

C.stds.table # print d13C standards table
```


### Offset Correction
Apply basic offset correction to all of the data; does NOT include linearity or drift correction to data - culling any additional standards that should be culled (e.g. 2 sd outliers)
```{r offset.plot}
offsetC<-subset(stds2, sample_type=="drift.std" | sample_type=="dis.std.1")

(offsetC.mean<-mean(offsetC$d13C.measured))
(offsetC.sd<-sd(offsetC$d13C.measured))

offsetC$d13C.offset <- offsetC$d13C.measured +  (C.acc - offsetC.mean)

(offsetcorrC.mean<-mean(offsetC$d13C.offset))
(offsetcorrC.sd<-sd(offsetC$d13C.offset))

d13C.offset<-ggplot(offsetC, aes(x=ampl44_mean, y=d13C.offset, shape=file_id)) +
  geom_point(size=3) +
  geom_hline(yintercept=offsetcorrC.mean, colour="orange") +
  geom_hline(yintercept=offsetcorrC.mean + offsetcorrC.sd, colour="orange", linetype="dashed") +
  geom_hline(yintercept=offsetcorrC.mean - offsetcorrC.sd, colour="orange", linetype="dashed") +
  geom_hline(yintercept=offsetcorrC.mean + 2*offsetcorrC.sd, colour="orange", linetype=3) +
  geom_hline(yintercept=offsetcorrC.mean - 2*offsetcorrC.sd, colour="orange", linetype=3) +
  scale_shape_manual(values=c(18,19,20,21,22,23,24,25)) +
  annotate("text", y = offsetcorrC.mean + 0.01, x = min(offsetC$ampl44_mean), 
    label = paste0("mean: ", sprintf("%.2f", offsetcorrC.mean), " \U00B1 ", sprintf("%.2f", offsetcorrC.sd, 2), " \U2030 (1 sd)"),
    size = 4, hjust=0, vjust=0, parse=FALSE, colour="black") +
    scale_x_continuous(name = "m/z 44 peak amplitude (mV)") +
  scale_y_continuous(name = latex2exp::TeX("$\\delta$$^{13}$C offset corrected (‰)"))+
  theme_bw() 

d13C.offset
```

After finalizing the offset correction, apply it to the whole dataset, and check the monitoring standards
```{r offset.apply}

#apply offset correction to whole dataset
wo.culled$d13C.offset <- wo.culled$d13C.measured +  (C.acc - offsetC.mean)
stds2$d13C.offset <- stds2$d13C.measured +  (C.acc - offsetC.mean)

#make monitoring standard dataset and dataset for additional standards used later for discrimination correction
offsetC.mon <-subset(stds2, standard=="HIS")
offsetC.dis <- subset(stds2, standard=="LSVEC")

#check monitoring standard response
(offsetC.mon.mean<-mean(offsetC.mon$d13C.offset))
(offsetC.mon.sd<-sd(offsetC.mon$d13C.offset))

C.stds.table$offsetC.mean <- c(offsetcorrC.mean, offsetC.mon.mean, mean(offsetC.dis$d13C.offset))
C.stds.table$offsetC.sd <- c(offsetcorrC.sd, offsetC.mon.sd, sd(offsetC.dis$d13C.offset))

C.mon.offset<-ggplot(offsetC.mon, aes(x=ampl44_mean, y=d13C.offset)) +
  geom_point(shape=21, fill="orange") +
  geom_hline(yintercept=offsetC.mon.mean, colour="orange") +
  geom_hline(yintercept=offsetC.mon.mean + offsetC.mon.sd, colour="orange", linetype="dashed") +
  geom_hline(yintercept=offsetC.mon.mean - offsetC.mon.sd, colour="orange", linetype="dashed") +
  geom_hline(yintercept=offsetC.mon.mean + 2*offsetC.mon.sd, colour="orange", linetype=3) +
  geom_hline(yintercept=offsetC.mon.mean - 2*offsetC.mon.sd, colour="orange", linetype=3) +
  annotate("text", y = offsetC.mon.mean +0.01, x = min(offsetC.mon$ampl44_mean), label = paste0("mean: ", sprintf("%.2f", offsetC.mon.mean), " \U00B1 ", sprintf("%.2f", offsetC.mon.sd, 2), " \U2030 (1 sd)"), size = 4, hjust=0, vjust=0, parse=FALSE) +
      scale_x_continuous(name = "m/z 44 peak amplitude (mV)") +
  scale_y_continuous(name = latex2exp::TeX("$\\delta$$^{13}$C offset corrected (‰)"))+
  theme_bw()

C.mon.offset
```

### Drift corrections, using raw values
```{r drift.C}
driftC <- offsetC

drift.slopeC<-(coef(lm(driftC$d13C.measured ~ driftC$file_datetime_num))[[2]])
drift.interC<-(coef(lm(driftC$d13C.measured ~ driftC$file_datetime_num))[[1]])

#drift check
driftC$d13C.drift<- driftC$d13C.measured + (C.acc - (drift.slopeC * driftC$file_datetime_num + drift.interC))

(driftC.mean<-mean(driftC$d13C.drift))
(driftC.sd<-sd(driftC$d13C.drift))

C.drift.pre<-ggplot(driftC, aes(x=file_datetime_num, y=d13C.measured)) +
  geom_smooth(method=lm, colour="blue") +
   geom_point(shape=21, fill="black", size=2)+
  stat_poly_eq(aes(label =  paste(stat(eq.label), stat(rr.label), sep = "~~~~")),
               formula = driftC$d13C.measured ~ driftC$file_datetime_num, parse = TRUE, rr.digits = 6, color = "blue") +
  scale_x_continuous(name = "time of analysis (seconds since 1970-01-01)") +
  scale_y_continuous(name = latex2exp::TeX("$\\delta$$^{13}$C measured (‰)"))+
  theme_bw()

C.drift.post<-ggplot(driftC, aes(x=file_datetime_num, y=d13C.drift)) +
  geom_point(fill="red", shape=21, size=2) +
  geom_hline(aes(yintercept=C.acc), size=.5) +
  geom_hline(yintercept = driftC.mean + driftC.sd, colour="red", linetype="dashed") +
  geom_hline(yintercept = driftC.mean - driftC.sd, colour="red", linetype="dashed") +
  geom_hline(yintercept = driftC.mean + 2*driftC.sd, colour="red", linetype=3) +
  geom_hline(yintercept = driftC.mean - 2*driftC.sd, colour="red", linetype=3) +
  annotate("text", 
    y = driftC.mean +0.01, 
    x = min(driftC$file_datetime_num),
    label = paste0("mean: ", sprintf("%.2f", driftC.mean), " \U00B1 ", sprintf("%.2f", driftC.sd, 2), " \U2030 (1 sd)"),
    size = 4, hjust=0, vjust=0, parse=FALSE) +
    scale_x_continuous(name = "time of analysis (seconds since 1970-01-01)") +
  scale_y_continuous(name = latex2exp::TeX("$\\delta$$^{13}$C drift corrected (‰)"))+
    theme_bw()

C.drift.pre
C.drift.post
```

apply drift correction to all of the data, check with monitoring standards
```{r}
wo.culled$d13C.drift <- wo.culled$d13C.measured +  (C.acc - (drift.slopeC * wo.culled$file_datetime_num + drift.interC))
stds2$d13C.drift <- stds2$d13C.measured +  (C.acc - (drift.slopeC * stds2$file_datetime_num + drift.interC))

driftC.mon<-subset(stds2, standard=="HIS")
driftC.dis <- subset(stds2, standard=="LSVEC")

(driftC.mon.mean<-mean(driftC.mon$d13C.drift))
(driftC.mon.sd<-sd(driftC.mon$d13C.drift))

C.stds.table$driftC.mean <- c(driftC.mean, driftC.mon.mean, mean(driftC.dis$d13C.drift))
C.stds.table$driftC.sd <- c(driftC.sd, driftC.mon.sd, sd(driftC.dis$d13C.drift))

C.mon.drift<-ggplot(driftC.mon, aes(x=ampl44_mean, y=d13C.drift)) +
  geom_point(shape=21, fill="red") +
  geom_hline(yintercept = driftC.mon.mean, colour="red") +
  geom_hline(yintercept = driftC.mon.mean + driftC.mon.sd, colour="red", linetype="dashed") +
  geom_hline(yintercept = driftC.mon.mean - driftC.mon.sd, colour="red", linetype="dashed") +
  geom_hline(yintercept = driftC.mon.mean + 2*driftC.mon.sd, colour="red", linetype=3) +
  geom_hline(yintercept = driftC.mon.mean - 2*driftC.mon.sd, colour="red", linetype=3) +
  annotate("text",
    y = driftC.mon.mean +0.01, 
    x = min(driftC.mon$ampl44_mean),
    label = paste0("mean: ", sprintf("%.2f", driftC.mon.mean), " \U00B1 ", sprintf("%.2f", driftC.mon.sd, 2), " \U2030 (1 sd)"),
    size = 4, hjust=0, vjust=0, parse=FALSE, colour="red")+
    scale_x_continuous(name = "m/z 44 peak amplitude (mV)") +
  scale_y_continuous(name = latex2exp::TeX("$\\delta$$^{13}$C drift corrected (‰)"))+
      theme_bw()

C.mon.drift
```

### Linearity Corrections
Regular m/z 44 amplitude is better correlated with $\delta^{13}$C than inverse m/z 44 amplitude, so I went with the regular amplitude for the correction.
```{r linearityC}
linC<-subset(stds2, sample_type=="lin.std") 

lin.slopeC<-(coef(lm(linC$d13C.measured ~ linC$ampl44_mean))[[2]])
lin.interC<-(coef(lm(linC$d13C.measured ~ linC$ampl44_mean))[[1]])

#linearity check
linC$d13C.lin<-linC$d13C.measured + (C.acc - (lin.slopeC * linC$ampl44_mean + lin.interC))

(linC.mean<-mean(linC$d13C.lin))
(linC.sd<-sd(linC$d13C.lin))

C.lincorr.pre<-ggplot(linC, aes(x=ampl44_mean, y=d13C.measured)) +
  geom_smooth(method=lm, colour="blue") +
   geom_point(shape=21, fill="black", size=2)+
  stat_poly_eq(aes(label =  paste(stat(eq.label), stat(rr.label), sep = "~~~~")), formula = linC$d13C.measured ~ linC$ampl44_mean, parse = TRUE, rr.digits = 6, color = "blue") +
  scale_x_continuous(name = "m/z 44 amplitude (mV)") +
  scale_y_continuous(name = latex2exp::TeX("$\\delta$$^{13}$C measured (‰)"))+
  theme_bw()

C.lincorr.pre.inv<-ggplot(linC, aes(x=inv.ampl44, y=d13C.measured)) +
  geom_smooth(method=lm, colour="blue") +
   geom_point(shape=21, fill="black", size=2)+
  stat_poly_eq(aes(label =  paste(stat(eq.label), stat(rr.label), sep = "~~~~")), formula = linC$d13C.measured ~ linC$inv.ampl44, parse = TRUE, rr.digits = 6, color = "blue") +
  scale_x_continuous(name = "inverse m/z 44 amplitude (mV)") +
  scale_y_continuous(name = latex2exp::TeX("$\\delta$$^{13}$C measured (‰)"))+
  theme_bw()

C.lincorr.post<-ggplot(linC, aes(x=ampl44_mean, y=d13C.lin)) +
  geom_point(fill="red", shape=21, size=2) +
  geom_hline(aes(yintercept=C.acc), size=.5) +
  geom_hline(yintercept = linC.mean + linC.sd, colour="red", linetype="dashed") +
  geom_hline(yintercept = linC.mean - linC.sd, colour="red", linetype="dashed") +
  geom_hline(yintercept = linC.mean + 2*linC.sd, colour="red", linetype=3) +
  geom_hline(yintercept = linC.mean - 2*linC.sd, colour="red", linetype=3) +
  annotate("text", 
    y = linC.mean +0.01, 
    x = min(linC$ampl44_mean),
    label = paste0("mean: ", sprintf("%.2f", linC.mean), " \U00B1 ", sprintf("%.2f", linC.sd, 2), " \U2030 (1 sd)"),
    size = 4, hjust=0, vjust=0, parse=FALSE) +
    scale_x_continuous(name = "m/z 44 amplitude (mV)") +
  scale_y_continuous(name = latex2exp::TeX("$\\delta$$^{13}$C linearity corrected (‰)"))+
    theme_bw()

C.lincorr.pre
C.lincorr.pre.inv
C.lincorr.post
```

apply linearity correction + offset correction to all of the data
```{r apply.lin.corr.C}

wo.culled$d13C.lin <- wo.culled$d13C.measured +  (C.acc - (lin.slopeC * wo.culled$ampl44_mean + lin.interC))
stds2$d13C.lin <- stds2$d13C.measured +  (C.acc - (lin.slopeC * stds2$ampl44_mean + lin.interC))

linC.mon<-subset(stds2, standard== "HIS")
linC.dis<-subset(stds2, standard=="LSVEC")

(linC.mon.mean <- mean(linC.mon$d13C.lin))
(linC.mon.sd<-sd(linC.mon$d13C.lin))

C.stds.table$linC.mean <- c(linC.mean, linC.mon.mean, mean(linC.dis$d13C.lin))
C.stds.table$linC.sd <- c(linC.sd, linC.mon.sd, sd(linC.dis$d13C.lin))

C.mon.lin<-ggplot(linC.mon, aes(x=ampl44_mean, y=d13C.lin)) +
  geom_point(shape=21, fill="blue") +
  geom_hline(yintercept = linC.mon.mean, colour="blue") +
  geom_hline(yintercept = linC.mon.mean + linC.mon.sd, colour="blue", linetype="dashed") +
  geom_hline(yintercept = linC.mon.mean - linC.mon.sd, colour="blue", linetype="dashed") +
  geom_hline(yintercept = linC.mon.mean + 2*linC.mon.sd, colour="blue", linetype=3) +
  geom_hline(yintercept = linC.mon.mean - 2*linC.mon.sd, colour="blue", linetype=3) +
  annotate("text",
    y = linC.mon.mean +0.01, 
    x = min(linC.mon$ampl44_mean),
    label = paste0("mean: ", sprintf("%.2f", linC.mon.mean), " \U00B1 ", sprintf("%.2f", linC.mon.sd, 2), " \U2030 (1 sd)"),
    size = 4, hjust=0, vjust=0, parse=FALSE, colour="blue") +
    scale_x_continuous(name = "m/z 44 amplitude (mV)") +
  scale_y_continuous(name = latex2exp::TeX("$\\delta$$^{13}$C linearity corrected (‰)"))+
    theme_bw()

C.mon.lin
```

### Drift corrections on the linearity corrected values
```{r lindrift.C}

lindriftC <- stds2 %>% filter(standard=="YULE")

lindrift.slopeC<-(coef(lm(lindriftC$d13C.lin ~ lindriftC$file_datetime_num))[[2]])
lindrift.interC<-(coef(lm(lindriftC$d13C.lin ~ lindriftC$file_datetime_num))[[1]])

#drift check
lindriftC$d13C.lindrift<- lindriftC$d13C.lin + (C.acc - (lindrift.slopeC * lindriftC$file_datetime_num + lindrift.interC))

(lindriftC.mean<-mean(lindriftC$d13C.drift))
(lindriftC.sd<-sd(lindriftC$d13C.drift))

C.lindrift<-ggplot(lindriftC, aes(x=file_datetime_num, y=d13C.lin)) +
  geom_smooth(method=lm, colour="blue") +
  geom_point(shape=21, fill="blue", size=2) +
   stat_poly_eq(aes(label =  paste(stat(eq.label), stat(rr.label), sep = "~~~~")), formula = lindriftC$d13C.lin ~ lindriftC$file_datetime_num, parse = TRUE, rr.digits = 6, color = "blue") +
  geom_point(aes(x=file_datetime_num, y=d13C.lindrift), fill="red", shape=22, size=2) +
  geom_hline(aes(yintercept=C.acc), size=.5) +
  geom_hline(yintercept = lindriftC.mean + lindriftC.sd, colour="red", linetype="dashed") +
  geom_hline(yintercept = lindriftC.mean - lindriftC.sd, colour="red", linetype="dashed") +
  geom_hline(yintercept = lindriftC.mean + 2*lindriftC.sd, colour="red", linetype=3) +
  geom_hline(yintercept = lindriftC.mean - 2*lindriftC.sd, colour="red", linetype=3) +
  annotate("text",
    y = lindriftC.mean +0.01, 
    x = min(lindriftC$file_datetime_num),
    label = paste0("mean: ", sprintf("%.2f", lindriftC.mean), " \U00B1 ", sprintf("%.2f", lindriftC.sd, 2), " \U2030 (1 sd)"),
    size = 4, hjust=0, vjust=0, parse=FALSE) +
    scale_x_continuous(name = "time of analysis (seconds since 1970-01-01)") +
   scale_y_continuous(name = latex2exp::TeX("$\\delta$$^{13}$C linearity corrected (‰)"))+
    theme_bw()

C.lindrift
```

apply drift correction to all of the linearity corrected data, check with monitoring standards
```{r apply.lindrift.corr.C}
wo.culled$d13C.lindrift <- wo.culled$d13C.lin +  (C.acc - (lindrift.slopeC * wo.culled$file_datetime_num + lindrift.interC))
stds2$d13C.lindrift <- stds2$d13C.lin +  (C.acc - (lindrift.slopeC * stds2$file_datetime_num + lindrift.interC))

lindriftC.mon<-subset(stds2, standard=="HIS")
lindriftC.dis<-subset(stds2, standard=="LSVEC")

(lindriftC.mon.mean<-mean(lindriftC.mon$d13C.lindrift))
(lindriftC.mon.sd<-sd(lindriftC.mon$d13C.lindrift))

C.stds.table$lindriftC.mean <- c(lindriftC.mean, lindriftC.mon.mean, mean(lindriftC.dis$d13C.lindrift))
C.stds.table$lindriftC.sd <- c(lindriftC.sd, lindriftC.mon.sd, sd(lindriftC.dis$d13C.lindrift))

C.mon.lindrift<-ggplot(lindriftC.mon, aes(x=ampl44_mean, y=d13C.lindrift)) +
  geom_point(shape=21, fill="red") +
  geom_hline(yintercept = lindriftC.mon.mean, colour="red") +
  geom_hline(yintercept = lindriftC.mon.mean + lindriftC.mon.sd, colour="red", linetype="dashed") +
  geom_hline(yintercept = lindriftC.mon.mean - lindriftC.mon.sd, colour="red", linetype="dashed") +
  geom_hline(yintercept = lindriftC.mon.mean + 2*lindriftC.mon.sd, colour="red", linetype=3) +
  geom_hline(yintercept = lindriftC.mon.mean - 2*lindriftC.mon.sd, colour="red", linetype=3) +
  annotate("text",
    y = lindriftC.mon.mean +0.01, 
    x = min(lindriftC.mon$ampl44_mean),
    label = paste0("mean: ", sprintf("%.2f", lindriftC.mon.mean), " \U00B1 ", sprintf("%.2f", lindriftC.mon.sd, 2), " \U2030 (1 sd)"),
    size = 4, hjust=0, vjust=0, parse=FALSE, colour="red")+
      scale_x_continuous(name = "m/z 44 amplitude (mV)") +
  scale_y_continuous(name = latex2exp::TeX("$\\delta$$^{13}$C linearity and drift corrected (‰)"))+
  theme_bw()

C.mon.lindrift
```

### Isotopic Discrimination Corrections

Apply isotopic discrimination correction, then plot.

Here, I use the linearity corrected data for downstream analysis, since there was no apparent correlation of $\delta^{13}$C with time.
```{r disc.corrections, warning=FALSE}
# replace the character string here with the correction column you want to use
col_for_disc <- "lin"  # options to substitute in here are: "offset" or "drift" or  "lin" or  "lindrift".

# resulting formulas
col_in_C.stds.table <- paste0(col_for_disc, "C.mean") # name the column in the standards table
col_in_data <- paste0("d13C.", col_for_disc) # name the column in the samples dataframe
correction_eq <- list(d13C.disc = lazyeval::interp(~disc.slopeC * var + disc.interC, var = as.name(col_in_data))) # make equation for discrimation correction of samples
correction_eq_stds <- list(d13C.disc = lazyeval::interp(~disc.slopeC * var + disc.interC, var = as.name(col_in_C.stds.table))) # make equation for discrimation correction applied to standards as a check

# safety checks
if (!col_in_C.stds.table %in% names(C.stds.table)) stop("this column does not exist in C.stds.table: ", col_in_C.stds.table, call. = FALSE)
if (!col_in_data %in% names(wo.culled)) stop("this column does not exist in data or stds table: ", col_in_data, call. = FALSE)

# regression
reg_formula <- C.stds.table$C.acc ~C.stds.table[[col_in_C.stds.table]]
m <- lm(reg_formula)
disc.slopeC<-(coef(m)[[2]])
disc.interC<-(coef(m)[[1]])

# apply correction
wo.culled <- mutate_(wo.culled, .dots = correction_eq)
C.stds.table <- mutate_(C.stds.table, .dots = correction_eq_stds)

samples_only <- wo.culled %>% filter(sample_type == "sample") # filter out standards from the true samples

# create plot showing the standards with desired pre-discrimination corrected column vs. accepted values
C.pre.disc <- 
  ggplot(C.stds.table, aes_string(x="C.acc", y= col_in_C.stds.table,label = "std.name")) +
  geom_smooth(method="lm", color = "blue") +
  geom_point(shape=21, fill="blue", size = 4) +
  geom_label(nudge_x = 0, nudge_y = -3, aes(hjust = "left"), alpha=0.7)+
   stat_poly_eq(aes(label =  paste(stat(eq.label), stat(rr.label), sep = "~~~~")),
               formula = C.stds.table[[col_in_C.stds.table]] ~ C.stds.table$C.acc, parse = TRUE, rr.digits = 6, color = "blue") +
scale_x_continuous(name = latex2exp::TeX("$\\delta^{13}$C accepted (‰ VPDB)"), expand = c(0.2, 0.2))+
scale_y_continuous(name = str_interp("d13C ${col_for_disc} corrected (‰)"))+
theme_bw()

# create plot showing the standards discrimination corrected values vs. accepted values (should be exactly 1 to 1)
C.disc.acc <- 
  ggplot(C.stds.table, aes_string(x="C.acc", y="d13C.disc",label = "std.name")) +
  geom_smooth(method="lm", color = "blue") +
  geom_point(shape=21, fill="blue", size = 4) +
  geom_label(nudge_x = 0, nudge_y = -3, aes(hjust = "left"), alpha=0.7)+
  stat_poly_eq(aes(label =  paste(stat(eq.label), stat(rr.label), sep = "~~~~")),
               formula = C.stds.table$d13C.disc ~ C.stds.table$C.acc, parse = TRUE, rr.digits = 6, color = "blue")+
scale_x_continuous(name = latex2exp::TeX("$\\delta^{13}$C accepted (‰ VPDB)"), expand = c(0.2, 0.2))+
scale_y_continuous(name = latex2exp::TeX("$\\delta^{13}$C discrimination corrected (‰ VPDB)"))+
theme_bw()

# create plot showing the corrected values. standards are in blue; samples are in red
C.disc.all <- 
  ggplot(C.stds.table, aes_string(x="d13C.disc", y=col_in_C.stds.table)) +
  geom_smooth(method="lm", color = "blue") +
  geom_point(shape=21, fill="blue", size = 4) +
  stat_poly_eq(aes(label =  paste(stat(eq.label), stat(rr.label), sep = "~~~~")),
               formula = C.stds.table[[col_in_C.stds.table]] ~ C.stds.table$d13C.disc, parse = TRUE, rr.digits = 6, color = "blue") +
  geom_point(data=samples_only, aes_string(x="d13C.disc", y=col_in_data), shape=23, fill="red", size = 2) +
scale_x_continuous(name = latex2exp::TeX("$\\delta^{13}$C discrimination corrected (‰ VPDB)")) +
scale_y_continuous(name = str_interp("d13C ${col_for_disc} corrected (‰)"))+
  theme_bw()

C.pre.disc # plot standards before discrimination correction

C.disc.acc # plot discrimination correction vs. accepted values for standards. Values should be nearly identical.

C.disc.all # plot discrimination corrected values including samples
```

```{r add_disc_to_stds.mean.table}
discC.mon<-subset(wo.culled, sample_type=="mon.std" | sample_type=="dis.std.2")
discC.dis<-subset(wo.culled, sample_type=="dis.std.3")
discC.corr<-subset(wo.culled, sample_type=="lin.std" | sample_type== "drift.std" | sample_type== "dis.std.1")

(discC.mon.mean<-mean(discC.mon$d13C.disc))
(discC.mon.sd<-sd(discC.mon$d13C.disc))

C.stds.table$discC.mean <- c(mean(discC.corr$d13C.disc), discC.mon.mean, mean(discC.dis$d13C.disc))
C.stds.table$discC.sd <- c(sd(discC.corr$d13C.disc), discC.mon.sd, sd(discC.dis$d13C.disc))
```

## Export data
Add sample names to isotope data
```{r clean.up.data.for.export}
samples_only <- samples_only  %>% mutate(sample_name = case_when(
  str_detect(file_id, "16591__5 min He purge 190104") == TRUE ~ "5 min He purge 190104",
  file_id == "16593__5 min He purge 190104 + 100 ul H3PO4 190104 rep1.dxf" ~ "100 ul H3PO4 190104",
  file_id == "16595__5 min He purge 190104 + 100 ul H3PO4 190104 rep3.dxf" ~ "100 ul H3PO4 190104",
  analysis == 16597 ~ "1 ml boiled MilliQ 190104",
  analysis == 16617 ~ "1 ml boiled MilliQ 190104 + 194ug YULE",
  str_detect(file_id, "WAB71") == TRUE ~ "WAB71",
  str_detect(file_id, "BA1A@30") == TRUE ~ "BA1A<30",
  str_detect(file_id, "WAB105") == TRUE ~ "WAB105",
  str_detect(file_id, "NSHQ14") == TRUE ~ "NSHQ14",
  str_detect(file_id, "BA1A4165") == TRUE ~ "BA1A4165",
  str_detect(file_id, "BA1A108132") == TRUE ~ "BA1A108132",
  str_detect(file_id, "LT17-W5") == TRUE ~ "LT17-W5",
  str_detect(file_id, "LT17-W6") == TRUE ~ "LT17-W6",
  str_detect(file_id, "LT17-W3") == TRUE ~ "LT17-W3",
  str_detect(file_id, "LOQ") == TRUE ~ "LOQ"
))

isotope_samples_summ <- samples_only %>% group_by(sample_name) %>% summarise(d13C_mean_permil_vpdb = mean(d13C.disc), d13C_sd_permil = sd(d13C.disc))

all_samples <- isotope_samples_summ %>% full_join(samples_select, by = "sample_name")

all_samples <- all_samples %>% select(sample_name, n, ampl44_t0, LOQ, DIC_uM, DIC_uM_95_confidence, d13C_mean_permil_vpdb, d13C_sd_permil)

all_samples %>% kable()
```
Prepare standards for export.
```{r prep.stnds.for.export}
stnds_for_export <- discC.mon %>% bind_rows(discC.dis) %>% bind_rows(discC.corr) # combine corrected standards

# add standard names
stnds_for_export <- stnds_for_export %>% mutate(standard = case_when(
    str_detect(file_id, "YULE") == TRUE ~ "YULE",
    str_detect(file_id, "HIS") == TRUE ~ "HIS",
    str_detect(file_id, "LSVEC") == TRUE ~ "LSVEC",
    str_detect(file_id, "NBS19") == TRUE ~ "NBS19"
))

stnds_for_export <- stnds_for_export %>% select(file_id, sample_type, standard, file_datetime, file_datetime_num, ampl44_mean, d13C.offset, d13C.drift, d13C.lin, d13C.lindrift, d13C.disc) # make simplified dataframe for monitor standards

# add standard accepted delta values
stnds_for_export <- stnds_for_export %>% mutate(d13C.acc = case_when(
    standard == "YULE" ~ -2.28,
    standard == "HIS" ~ -4.80,
    standard == "LSVEC" ~ -46.6,
    standard == "NBS19" ~ 1.95
))

# add some metadata for long-term standard monitoring
stnds_for_export <- stnds_for_export %>% mutate(operator = operator) %>% mutate(session = session) %>%  mutate(vol_H2O_sample_ml)
```


```{r export.data}
# make function to add a worksheet with data
add_ws_with_data <- function(wb, sheet, data) {
  addWorksheet(wb, sheet)
  writeData(wb, sheet=sheet, data)
  return(wb)
}

# creat a workbook and add samples + monitor standards
wb <- createWorkbook() 
wb <- add_ws_with_data(wb, "samples_summ", all_samples)
wb <- add_ws_with_data(wb, "standards", stnds_for_export)
wb <- add_ws_with_data(wb, "all_wo_culled", wo.culled)

# save workbook to .xlsx file
saveWorkbook(wb, file.path("~/Google_Drive/Delta methods/GasBench Stuff/Dan/190208_OM19_1ml_DIC", paste0(session, "_corrected_data.xlsx")), overwrite = TRUE)
```